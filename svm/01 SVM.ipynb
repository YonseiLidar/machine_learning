{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM\n",
    " \n",
    "Sungchul Lee "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Sebastian Thrun - Intro to Machine Learning 67--108 (Udacity)\n",
    "\n",
    "- Abhishek Ghose\n",
    "[Support Vector Machine (SVM) Tutorial](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93)\n",
    "\n",
    "- Ajay Unagar \n",
    "[Support Vector Machines. Unwinded.](https://medium.com/data-science-group-iitr/support-vector-machines-svm-unraveled-e0e7e3ccd49b)\n",
    "\n",
    "- CHRISTOPHER J.C. BURGES - A Tutorial on Support Vector Machines for Pattern Recognition \n",
    "[local](http://localhost:8888/notebooks/Dropbox/Paper/A Tutorial on Support Vector Machines for Pattern Recognition.pdf)\n",
    "\n",
    "- Dustin Boswell - An introduction to support vector machines \n",
    "[local](http://localhost:8888/notebooks/Dropbox/Paper/An introduction to support vector machines.pdf)\n",
    "\n",
    "- Geoff Gordon - Support Vector Machines and Kernel Methods \n",
    "[local](http://localhost:8888/notebooks/Dropbox/Paper/Support Vector Machines and Kernel Methods.pdf)\n",
    "\n",
    "- Elements of Statistical Learning - Chapter 9: Support Vector Machines\n",
    "[9.1 Maximal Margin Classifier](https://www.youtube.com/watch?v=QpbynqiTCsY&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "[9.2 Support Vector Classifier](https://www.youtube.com/watch?v=xKsTsGE7KpI&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "[9.3 Kernels and Support Vector Machines](https://www.youtube.com/watch?v=dm32QvCW7wE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "[9.4 Example and Comparison with Logistic Regression](https://www.youtube.com/watch?v=mI18GD4_ysE&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "[9.5 Lab: Support Vector Machine for Classification](https://www.youtube.com/watch?v=qhyyufR0930&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "[9.6 Lab: Nonlinear Support Vector Machine](https://www.youtube.com/watch?v=L3n2VF7yKkk&amp;list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o)\n",
    "\n",
    "- Andrew Ng - CS 229\n",
    "[note](http://cs229.stanford.edu/materials.html)\n",
    "[6](https://www.youtube.com/watch?v=qyyJKd-zXRE)\n",
    "[7](https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "[8](https://www.youtube.com/watch?v=bUv9bfMPMb4) \n",
    "\n",
    "- sentdex - Practical Machine Learning Tutorial with Python\n",
    "[20 Support Vector Machine Intro and Application](https://www.youtube.com/watch?v=mA5nwGoRAOo&index=20&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) \n",
    "[blog](https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/)\n",
    "[21 Understanding Vectors](https://www.youtube.com/watch?v=HHUqhVzctQE&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=21) \n",
    "[blog](https://pythonprogramming.net/vector-basics-machine-learning-tutorial/)\n",
    "[22 Support Vector Assertion](https://www.youtube.com/watch?v=VngCRWPrNNc&index=22&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) \n",
    "[blog](https://pythonprogramming.net/support-vector-assertions-machine-learning-tutorial/)\n",
    "[23 Support Vector Machine Fundamentals](https://www.youtube.com/watch?v=ZDu3LKv9gOI&index=23&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) \n",
    "[blog](https://pythonprogramming.net/support-vector-machine-fundamentals-machine-learning-tutorial/)\n",
    "[24 Support Vector Machine Optimization](https://www.youtube.com/watch?v=bGCafQT5h1s&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=24) \n",
    "[blog](https://pythonprogramming.net/svm-constraint-optimization-machine-learning-tutorial/)\n",
    "[25 Creating an SVM from scratch](https://www.youtube.com/watch?v=AbVtcUBlBok&index=25&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) \n",
    "[blog](https://pythonprogramming.net/svm-in-python-machine-learning-tutorial/)\n",
    "[26 SVM Training](https://www.youtube.com/watch?v=QAs2olt7pJ4&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=26) \n",
    "[blog](https://pythonprogramming.net/svm-optimization-python-machine-learning-tutorial/)\n",
    "[27 SVM Optimization](https://www.youtube.com/watch?v=VhHLpg7ZS4Q&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=27) [blog](https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/)\n",
    "[28 Completing SVM from Scratch](https://www.youtube.com/watch?v=yrnhziJk-z8&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=28) \n",
    "[blog](https://pythonprogramming.net/predictions-svm-machine-learning-tutorial/)\n",
    "[29 Kernels Introduction](https://www.youtube.com/watch?v=9IfT8KXX_9c&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=29) \n",
    "[blog](https://pythonprogramming.net/kernels-with-svm-machine-learning-tutorial/)\n",
    "[30 Why Kernels](https://www.youtube.com/watch?v=xqg5S-GrrDQ&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=30) \n",
    "[blog](https://pythonprogramming.net/why-use-kernel-with-svm-machine-learning-tutorial/)\n",
    "[31 Soft Margin SVM](https://www.youtube.com/watch?v=JHaqodAQqiI&index=31&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) [blog](https://pythonprogramming.net/soft-margin-svm-machine-learning-tutorial/)\n",
    "[32 Soft Margin SVM and Kernels with CVXOPT](https://www.youtube.com/watch?v=XdcfJX-mDG4&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=32) \n",
    "[blog](https://pythonprogramming.net/soft-margin-kernel-cvxopt-svm-machine-learning-tutorial/)\n",
    "[33 SVM Parameters](https://www.youtube.com/watch?v=93AjE1YY5II&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=33) [blog](https://pythonprogramming.net/support-vector-machine-parameters-machine-learning-tutorial/)\n",
    "\n",
    "- Jake VanderPlas\n",
    "[SVM](https://github.com/jakevdp/sklearn_tutorial/blob/master/notebooks/03.1-Classification-SVMs.ipynb)\n",
    "\n",
    "- James Bergstra\n",
    "[Linear SVM](https://github.com/jaberg/IPythonTheanoTutorials/blob/master/ipynb/Model%20-%20Linear%20SVM%20with%20PyAutodiff.ipynb)\n",
    "\n",
    "- Jason Weston \n",
    "[Support Vector Machine (and Statistical Learning Theory) Tutorial](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to run these slides yourself\n",
    "\n",
    "**Setup python environment**\n",
    "\n",
    "- Install RISE for an interactive presentation viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.11.42 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.12.25 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.13.47 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.14.32 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.15.22 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.15.55 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.26.16 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.27.03 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.27.36 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.29.02 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 4.29.27 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 5.28.54 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2018-02-07 at 5.30.07 AM.png\" width=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "$$\n",
    "\\mbox{Linear Decision Boundary}\n",
    "\\quad\\stackrel{\\mbox{Kernel Trick}}{\\Rightarrow}\\quad\n",
    "\\mbox{Non-Linear Decision Boundary}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||Logistic regression|SVM|\n",
    "|---|---|---|\n",
    "|Decision boundary|Linear|Non-linear|                                 \n",
    "|Notation|$\\displaystyle{\\bf x}^T{\\bf\\theta}$|$\\displaystyle{\\bf x}^T{\\bf\\theta}+b$|                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin\n",
    "\n",
    "We assume all points can be correctly specified.\n",
    "\n",
    "### Hyperplane $P$\n",
    "$$\n",
    "P:\\ {\\bf x}^T{\\bf\\theta}+b=0\n",
    "$$\n",
    "\n",
    "### Distance $d(P,{\\bf x}^{(i)})$ between a hyperplane $P$ and a  point ${\\bf x}^{(i)}$ using the absolute value\n",
    "\n",
    "$$\n",
    "d(P,{\\bf x}^{(i)})=\\left\\{\\begin{array}{ll}\n",
    "\\left|{\\bf x}^{(i) T}{\\bf\\theta}+b\\right|&\\mbox{if}\\ \\|{\\bf\\theta}\\|=1\\\\\n",
    "\\\\\n",
    "\\left|{\\bf x}^{(i) T}\\frac{{\\bf\\theta}}{\\|{\\bf\\theta}\\|}+\\frac{b}{\\|{\\bf\\theta}\\|}\\right|=\\left|{\\bf x}^{(i) T}\\frac{{\\bf\\theta}}{\\|{\\bf\\theta}\\|}+b'\\right|&\\mbox{in general}\\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "### Distance $d(P,{\\bf x}^{(i)})$ between $P$ and a  point ${\\bf x}^{(i)}$ using the label $y^{(i)}=\\pm1$\n",
    "$$\n",
    "d(P,{\\bf x}^{(i)})=\\left\\{\\begin{array}{ll}\n",
    "y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)&\\mbox{if}\\ \\|{\\bf\\theta}\\|=1\\\\\n",
    "\\\\\n",
    "y^{(i)}\\left({\\bf x}^{(i) T}\\frac{{\\bf\\theta}}{\\|{\\bf\\theta}\\|}+b'\\right)&\\mbox{in general}\\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin\n",
    "$$\n",
    "\\gamma_{{\\bf\\theta},b'}\n",
    "\\ \\ =\\ \\ \n",
    "\\mbox{min}_{i}\\ \\ \n",
    "y^{(i)}\\left({\\bf x}^{(i) T}\\frac{{\\bf\\theta}}{\\|{\\bf\\theta}\\|}+b'\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mbox{A point ${\\bf x}_0$ on the hyperplane $P$}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\  {\\bf x}_0^T{\\bf\\theta}+b=0\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "d(P,{\\bf x}^{(i)})=\\left|\\left({\\bf x}^{(i)}-{\\bf x}_0\\right)^T{\\bf\\theta}\\right|\n",
    "=\\left|\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)-\\left({\\bf x}_0^T{\\bf\\theta}+b\\right)\\right|\n",
    "=\\left|{\\bf x}^{(i) T}{\\bf\\theta}+b\\right|\n",
    "\\quad\\mbox{if}\\ \\|{\\bf\\theta}\\|=1\n",
    "$$\n",
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"img/sdviHIQHEeessdRE.png\" width=\"50%\"></div>\n",
    "\n",
    "http://www.xbdev.net/maths\\_of\\_3d/collision\\_detection/point\\_to\\_plane/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal margin classifier - Primal\n",
    "We assume all points can be correctly specified.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mbox{First try}&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{{\\bf\\theta},b,\\alpha}&&&\\alpha\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&\\|{\\bf\\theta}\\|=1\\quad\\leftarrow\\mbox{Nasty non-convex constraint}\\\\\n",
    "&&&y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\ge\\alpha\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\\nonumber\\\\\n",
    "\\nonumber\\\\\\nonumber\\\\\\nonumber\\\\\n",
    "\\mbox{Second try}&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{{\\bf\\theta},b,\\alpha}&&&\\frac{\\alpha}{\\|{\\bf\\theta}\\|}\n",
    "\\quad\\leftarrow\\mbox{Nasty non-convex objective function}\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\ge\\alpha\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\\nonumber\\\\\n",
    "\\nonumber\\\\\\nonumber\n",
    "\\mbox{Fix $\\alpha=1$}\\nonumber\\\\\n",
    "\\nonumber\\\\\\nonumber\n",
    "\\mbox{Quadratic program}&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{{\\bf\\theta},b}&&&\\frac{1}{2}\\|{\\bf\\theta}\\|^2\n",
    "\\quad\\leftarrow\\mbox{Convex objective function}\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\ge 1\\quad\\mbox{for all $i$}\n",
    "\\quad\\leftarrow\\mbox{Convex region}\\\\\n",
    "\\end{array}\\nonumber\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange multiplier\n",
    "\n",
    "### Original optimization problem\n",
    "$$\n",
    "\\begin{array}{cccccccccc}\n",
    "\\mbox{min}_{\\omega}&&&f(\\omega)\\\\\n",
    "\\mbox{subject to}&&&h_j(\\omega)=0&\\mbox{for $j=1,2,\\cdots,n$}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Lagrangian ${\\cal L}$ and Lagrange multiplier $\\beta=(\\beta_1,\\beta_2,\\cdots,\\beta_n)$\n",
    "$$\n",
    "{\\cal L}(\\omega,\\beta)\n",
    "=\n",
    "f(\\omega)+\\sum_{j=1}^n\\beta_jh_j(\\omega)\n",
    "$$\n",
    "\n",
    "### Theorem\n",
    "For $\\omega^*$ to be a solution of the original optimization problem\n",
    "it is necessary that there is $\\beta^*$ such that\n",
    "\\begin{eqnarray}\n",
    "(1)&&\\frac{\\partial{\\cal L}(\\omega^*,\\beta^*)}{\\partial\\omega}=0\\nonumber\\\\\n",
    "(2)&&\\frac{\\partial{\\cal L}(\\omega^*,\\beta^*)}{\\partial\\beta}=0\\nonumber\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal and dual\n",
    "\n",
    "### Original optimization problem\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{\\omega}&&&f(\\omega)\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&g_i(\\omega)\\le 0\\quad\\mbox{for $i=1,2,\\cdots,m$}\\\\\n",
    "                       &&&h_j(\\omega)=0\\quad\\mbox{for $j=1,2,\\cdots,n$}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Lagrangian ${\\cal L}$ and Lagrange multiplier $\\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_m)$, $\\beta=(\\beta_1,\\beta_2,\\cdots,\\beta_n)$\n",
    "$$\n",
    "{\\cal L}(\\omega,\\alpha,\\beta)\n",
    "\\quad=\\quad\n",
    "f(\\omega)\\quad+\\quad\\sum_{i=1}^N\\alpha_ig_i(\\omega)\\quad+\\quad\\sum_{j=1}^n\\beta_jh_j(\\omega)\n",
    "$$\n",
    "\n",
    "### Primal\n",
    "\\begin{eqnarray}\n",
    "P(\\omega)\n",
    "&:=&\n",
    "\\mbox{max}_{\\alpha\\ge 0,\\beta}\n",
    "\\quad\n",
    "{\\cal L}(\\omega,\\alpha,\\beta)\\nonumber\\\\\n",
    "\\nonumber\\\\\n",
    "P^*\n",
    "&:=&\n",
    "\\mbox{min}_{\\omega}\n",
    "\\quad\n",
    "P(\\omega)\n",
    "\\quad=\\quad\n",
    "\\mbox{min}_{\\omega}\n",
    "\\quad\n",
    "\\mbox{max}_{\\alpha\\ge 0,\\beta}\n",
    "\\quad\n",
    "{\\cal L}(\\omega,\\alpha,\\beta)\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Dual\n",
    "\\begin{eqnarray}\n",
    "D(\\alpha,\\beta)\n",
    "&:=&\n",
    "\\mbox{min}_{\\omega}\n",
    "\\quad\n",
    "{\\cal L}(\\omega,\\alpha,\\beta)\\nonumber\\\\\n",
    "\\nonumber\\\\\n",
    "D^*\n",
    "&:=&\n",
    "\\mbox{max}_{\\alpha\\ge 0,\\beta}\n",
    "\\quad\n",
    "D(\\alpha,\\beta)\n",
    "\\quad=\\quad\n",
    "\\mbox{max}_{\\alpha\\ge 0,\\beta}\n",
    "\\quad\n",
    "\\mbox{min}_{\\omega}\n",
    "\\quad\n",
    "{\\cal L}(\\omega,\\alpha,\\beta)\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Theorem\n",
    "\\begin{eqnarray}\n",
    "(1)&&\\mbox{Primal solves the original optimization problem}\\nonumber\\\\\n",
    "(2)&&P^*\\ge D^*\\nonumber\\\\\n",
    "(3)&&P^*=D^*\\quad\\mbox{if $f$ convex, $g_i$ convex, $h_j$ affine, $g_i\\le 0$ strictly feasible (see next)}\\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKT (Karush-Kuhn-Tucker) conditions\n",
    "\n",
    "### Original optimization problem\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{\\omega}&&&f(\\omega)\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&g_i(\\omega)\\le 0\\quad\\mbox{for $i=1,2,\\cdots,m$}\\\\\n",
    "                       &&&h_i(\\omega)=0\\quad\\mbox{for $i=1,2,\\cdots,n$}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Theorem\n",
    "If\n",
    "\\begin{eqnarray}\n",
    "(1)&&\\mbox{$f$ is convex}\\nonumber\\\\\n",
    "(2)&&\\mbox{$g_i$, $1\\le i\\le m$, are convex}\\nonumber\\\\\n",
    "(3)&&\\mbox{$h_j$, $1\\le j\\le n$, are affine, i.e., equality constraints can be written as ${\\bf A}\\omega={\\bf B}$}\\nonumber\\\\\n",
    "(4)&&\\mbox{$g_i\\le 0$, $1\\le i\\le m$, are strictly feasible, i.e.,}\\nonumber\\\\\n",
    "&&\\mbox{there is a feasible $\\omega$ such that $g_i(\\omega)<0$ for  all $1\\le i\\le m$}\\nonumber\n",
    "\\end{eqnarray}\n",
    "then $$P^*=D^*$$\n",
    "More explicitly, there exist $\\omega^*$, $\\alpha^*$, $\\beta^*$ such that\n",
    "\\begin{eqnarray}\n",
    "&&\\mbox{$\\omega^*$ solves primal}\\nonumber\\\\\n",
    "&&\\mbox{$\\alpha^*$ and $\\beta^*$ solve dual}\\nonumber\\\\\n",
    "&&P^*=D^*={\\cal L}(\\omega^*,\\alpha^*,\\beta^*)\\nonumber\n",
    "\\end{eqnarray}\n",
    "Furthermore, they satisfy\n",
    "\\begin{eqnarray}\n",
    "(1)&&\\frac{\\partial{\\cal L}(\\omega^*,\\alpha^*,\\beta^*)}{\\partial\\omega}=0\\nonumber\\\\\n",
    "(2)&&\\frac{\\partial{\\cal L}(\\omega^*,\\alpha^*,\\beta^*)}{\\partial\\beta}=0\\nonumber\\\\\n",
    "(3)&&\\alpha^*_ig_i(\\omega^*)=0\\quad\\leftarrow\\mbox{KKT complementary slackness condition}\\nonumber\\\\\n",
    "(4)&&\\alpha^*_i\\ge 0\\nonumber\\\\\n",
    "(5)&&g_i(\\omega^*)\\le 0\\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "(3),\\ (5)\n",
    "&\\Rightarrow&\n",
    "\\omega^*\\ \\mbox{feasible}\\nonumber\\\\\n",
    "(4)\\quad\\quad\n",
    "&\\Rightarrow&\n",
    "{\\cal L}\\ \\mbox{convex in $\\omega$}\\nonumber\\\\\n",
    "(1)\\quad\\quad\n",
    "&\\Rightarrow&\n",
    "{\\cal L}(\\omega^*,\\alpha^*,\\beta^*)\\ \\mbox{minimum in $\\omega$}\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$$\n",
    "D(\\alpha^*,\\beta^*)\n",
    "=\n",
    "{\\cal L}(\\omega^*,\\alpha^*,\\beta^*)=\n",
    "f(\\omega^*)+\\sum_{i=1}^N\\alpha_i^*g_i(\\omega^*)+\\beta^T({\\bf A}\\omega^*-{\\bf B})\n",
    "\\stackrel{\\mbox{(2), (3)}}{=}\n",
    "f(\\omega^*)\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow\\quad\n",
    "\\mbox{Zero duality gap!}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal margin classifier - Dual  formulation\n",
    "We assume all points can be correctly specified.\n",
    "\n",
    "\n",
    "### Quadratic program  formulation\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{{\\bf\\theta},b}&&&\\frac{1}{2}\\|{\\bf\\theta}\\|^2\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\ge 1\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\\nonumber\n",
    "\\end{eqnarray}\n",
    "Or\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{{\\bf\\theta},b}&&&\\frac{1}{2}\\|{\\bf\\theta}\\|^2\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&1-y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\le 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Lagrangian ${\\cal L}$\n",
    "$$\n",
    "{\\cal L}({\\bf\\theta},b,\\alpha)\n",
    "=\n",
    "\\frac{1}{2}\\|{\\bf\\theta}\\|^2+\\sum_{i=1}^N\\alpha_i\\left(1-y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\right)\n",
    "$$\n",
    "### KKT\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "\\frac{\\partial{\\cal L}({\\bf\\theta}^*,b^*,\\alpha)}{\\partial{\\bf\\theta}}={\\bf\\theta}^*-\\sum_{i=1}^N\\alpha_iy^{(i)}{\\bf x}^{(i)}={\\bf 0}\n",
    "\\quad\\Rightarrow\\quad\n",
    "{\\bf\\theta}^*=\\sum_{i=1}^N\\alpha_iy^{(i)}{\\bf x}^{(i)}\\nonumber\\\\\n",
    "&&\n",
    "\\frac{\\partial {\\cal L}({\\bf\\theta}^*,b^*,\\alpha)}{\\partial b}\n",
    "=-\\sum_{i=1}^N\\alpha_iy^{(i)}=0\n",
    "\\quad\\quad\\quad\\  \\Rightarrow\\quad\n",
    "\\sum_{i=1}^N\\alpha_iy^{(i)}=0\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Dual formulation\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy^{(i)}y^{(j)}{\\bf x}^{(i) T}{\\bf x}^{(j)}\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&\\alpha_i\\ge 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy^{(i)}=0\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}({\\bf\\theta}^*,b^*,\\alpha)\n",
    "&=&\n",
    "\\frac{1}{2}\\|{\\bf\\theta}^*\\|^2+\\sum_{i=1}^N\\alpha_i\\left(1-y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}^*+b^*\\right)\\right)\\nonumber\\\\\n",
    "&=&\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy^{(i)}y^{(j)}{\\bf x}^{(i) T}{\\bf X}^{(j)}\n",
    "+\\sum_{i=1}^N\\alpha_i-\\sum_{i=1}^N\\alpha_iy^{(i)}{\\bf x}^{(i) T}{\\bf\\theta}^*-\\sum_{i=1}^N\\alpha_iy^{(i)} b^*\\nonumber\\\\\n",
    "&=&\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy^{(i)}y^{(j)}{\\bf x}^{(i) T}{\\bf x}^{(j)}\n",
    "+\\sum_{i=1}^N\\alpha_i-\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy^{(i)}y^{(j)}{\\bf x}^{(i) T}{\\bf x}^{(j)}\\nonumber\\\\\n",
    "&=&\n",
    "\\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy^{(i)}y^{(j)}{\\bf x}^{(i) T}{\\bf x}^{(j)}\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal margin classifier - Primal  and Dual \n",
    "We assume all points can be correctly specified.\n",
    "\n",
    "### Primal\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{{\\bf\\theta},b}&&&\\frac{1}{2}\\|{\\bf\\theta}\\|^2\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&y^{(i)}\\left({\\bf x}^{(i) T}{\\bf\\theta}+b\\right)\\ge 1\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Dual\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j{\\bf x}^{(i) T}{\\bf x}^{(j)}\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&\\alpha_i\\ge 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy^{(i)}=0\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\gamma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector\n",
    "\n",
    "<div align=\"center\"><img src=\"img/sdviHIQHEee123ssdRE.png\" width=\"50%\"></div>\n",
    "From Andrew Ng's lecture note\n",
    "\n",
    "Solve the above quadratic program and find the optimal $\\alpha^*_i$.\n",
    "Then we have SVM.\n",
    "$$\n",
    "\\alpha^*_i\n",
    "\\ \\ \\Rightarrow\\ \\ \n",
    "{\\bf\\theta}^*=\\sum_{i=1}^N\\alpha^*_iy_i{\\bf X}_i%\\nonumber\\\\\n",
    "\\ \\ \\Rightarrow\\ \\ \n",
    "b^*\n",
    "=\n",
    "\\frac{\\mbox{max}_{y_i=-1}{\\bf X}_i^T{\\bf\\theta}^*+\\mbox{min}_{y_i=1}{\\bf X}_i^T{\\bf\\theta}^*}{2}\n",
    "$$\n",
    "Many $\\alpha_i^*$ are zeros and {\\color{red}only a few good $\\alpha_i^*$ are non-zero!}\n",
    "\\begin{eqnarray}\n",
    "\\alpha_i=0\n",
    "&\\Rightarrow&\n",
    "y_i\\left({\\bf X}_i^T{\\bf\\theta}+b\\right)\\ge 1\\nonumber\\\\\n",
    "0<\\alpha_i\n",
    "&\\Rightarrow&\n",
    "y_i\\left({\\bf X}_i^T{\\bf\\theta}+b\\right)= 1\n",
    "\\ \\ \\Leftrightarrow\\ \\\n",
    "\\mbox{Support vector ${\\bf X}_i$}\\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal margin classifier - Kernel  formulation\n",
    "We assume all points can be correctly specified.\n",
    "### Dual formulation\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j{\\bf x}_i^T{\\bf x}_j\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&\\alpha_i\\ge 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "### Kernel\n",
    "$$\n",
    "{\\bf x}_i,\\ {\\bf x}_j\n",
    "\\quad\\rightarrow\\quad\n",
    "\\phi({\\bf x}_i),\\ \\phi({\\bf x}_j) \n",
    "\\quad\\rightarrow\\quad\n",
    "K({\\bf x}_i,{\\bf x}_j)=<\\phi({\\bf x}_i),\\phi({\\bf x}_j) >\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mbox{Polynomial kernel}\\quad\\ &&(c+{\\bf x}_i^T{\\bf x}_j)^d\\nonumber\\\\\n",
    "\\mbox{Gaussian kernel}\\quad\\quad\\ &&e^{-\\frac{\\|{\\bf x}_i-{\\bf x}_j\\|^2}{2\\sigma^2}}\\nonumber\\\\\n",
    "\\mbox{Sigmoid kernel}\\quad\\quad\\ \\ &&\\tanh (a{\\bf x}_i^T{\\bf x}_j+b)\\nonumber\n",
    "\\end{eqnarray}\n",
    "### Kernel  formulation\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK({\\bf x}_i,{\\bf x}_j)\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&\\alpha_i\\ge 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "(1)&&\\mbox{Don't need know the feature space}\\nonumber\\\\\n",
    "(2)&&\\mbox{Works even if the feature space is infinite dimensional}\\nonumber\n",
    "\\end{eqnarray}\n",
    "### Theorem (Mercer)\n",
    "$K$ is a kernel if and only if\n",
    "for any $m$ inputs ${\\bf x}_i$\n",
    "the $m\\times m$ matrix $[K({\\bf x}_i,{\\bf x}_j)]$ is \n",
    "symmetric positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "K({\\bf x},{\\bf z})={\\bf x}^T{\\bf z}\n",
    "$$\n",
    "\n",
    "\n",
    "With $d=3$\n",
    "\\begin{eqnarray}\n",
    "K({\\bf x},{\\bf z})\n",
    "&=&\\left({\\bf x}^T{\\bf z}\\right)^2%\\nonumber\\\\\n",
    "=\\left(\\sum_{i=1}^dx_iz_i\\right)\\left(\\sum_{j=1}^dx_jz_j\\right)%\\nonumber\\\\\n",
    "=\\sum_{i=1}^d\\sum_{j=1}^dx_ix_jz_iz_j%\\nonumber\\\\\n",
    "=\\sum_{i,j=1}^d(x_ix_j)(z_iz_j)\\nonumber\\\\\n",
    "&=&\n",
    "\\left[\\begin{array}{c}x_1x_1\\\\x_1x_2\\\\x_1x_3\\\\x_2x_1\\\\x_2x_2\\\\x_2x_3\\\\x_3x_1\\\\x_3x_2\\\\x_3x_3\\\\\\end{array}\\right]^T\n",
    "\\left[\\begin{array}{c}z_1z_1\\\\z_1z_2\\\\z_1z_3\\\\z_2z_1\\\\z_2z_2\\\\z_2z_3\\\\z_3z_1\\\\z_3z_2\\\\z_3z_3\\\\\\end{array}\\right]\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "With $d=3$ and $c>0$\n",
    "\\begin{eqnarray}\n",
    "K({\\bf x},{\\bf z})\n",
    "&=&\\left({\\bf x}^T{\\bf z} +c\\right)^2%\\nonumber\\\\\n",
    "=\\left(\\sum_{i=1}^dx_iz_i +c\\right)\\left(\\sum_{j=1}^dx_jz_j +c\\right)\\nonumber\\\\\n",
    "&=&\n",
    "\\sum_{i,j=1}^d(x_ix_j)(z_iz_j)+\\sum_{i}^d(\\sqrt{2c}x_i)(\\sqrt{2c}z_i)+(c)(c)\\nonumber\\\\\n",
    "&=&\n",
    "\\left[\\begin{array}{c}x_1x_1\\\\x_1x_2\\\\x_1x_3\\\\x_2x_1\\\\x_2x_2\\\\x_2x_3\\\\x_3x_1\\\\x_3x_2\\\\x_3x_3\\\\\\sqrt{2c}x_1\\\\\\sqrt{2c}x_2\\\\\\sqrt{2c}x_3\\\\c\\end{array}\\right]^T\n",
    "\\left[\\begin{array}{c}z_1z_1\\\\z_1z_2\\\\z_1z_3\\\\z_2z_1\\\\z_2z_2\\\\z_2z_3\\\\z_3z_1\\\\z_3z_2\\\\z_3z_3\\\\\\sqrt{2c}z_1\\\\\\sqrt{2c}z_2\\\\\\sqrt{2c}z_3\\\\c\\end{array}\\right]\\nonumber\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L_1$ norm soft margin SVM\n",
    "\n",
    "We don't assume all points can be correctly specified.\n",
    "\n",
    "### Primal\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{min}_{{\\bf\\theta},b,\\xi}&&&\\frac{1}{2}\\|{\\bf\\theta}\\|^2+C\\sum_{i=1}^N\\xi_i\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&&y_i\\left({\\bf x}_i^T{\\bf\\theta}+b\\right)\\ge 1-\\xi_i\\quad\\mbox{for all $i$}\\\\\n",
    "&&&\\xi_i\\ge 0\\quad\\mbox{for all $i$}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "### Dual\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j{\\bf x}_i^T{\\bf x}_j\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&0\\le \\alpha_i\\le C\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "### Kernel\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "\\mbox{max}_{\\alpha}&&\n",
    "\\sum_{i=1}^N\\alpha_i\n",
    "-\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK({\\bf x}_i,{\\bf x}_j)\\\\\n",
    "\\\\\n",
    "\\mbox{subject to}&&0\\le \\alpha_i\\le C\\quad\\mbox{for all $i$}\\\\\n",
    "\\\\\n",
    "&&\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "### Support vector\n",
    "\\begin{eqnarray}\n",
    "\\alpha_i=0\n",
    "&\\Rightarrow&\n",
    "y_i\\left({\\bf x}_i^T{\\bf\\theta}+b\\right)\\ge 1\\nonumber\\\\\n",
    "\\alpha_i=C\n",
    "&\\Rightarrow&\n",
    "y_i\\left({\\bf x}_i^T{\\bf\\theta}+b\\right)\\le 1\\nonumber\\\\\n",
    "0<\\alpha_i<C\n",
    "&\\Rightarrow&\n",
    "y_i\\left({\\bf x}_i^T{\\bf\\theta}+b\\right)= 1\n",
    "\\ \\ \\Leftrightarrow\\ \\\n",
    "\\mbox{Support vector ${\\bf x}_i$}\\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMO (Sequential Minimal Optimization) algorithm\n",
    "\n",
    "Coordinate descent will violate the constraint $\\sum_{i=1}^N\\alpha_iy_i=0$.\n",
    "So, when you do a coordinate descent along a particular axis $i_0$,\n",
    "you should choose another axis $i_1$ so that \n",
    "$$\n",
    "\\alpha_{i_0}y_{i_0}\n",
    "+\n",
    "\\alpha_{i_1}y_{i_1}\n",
    "=\n",
    "-\n",
    "\\sum_{i\\neq i_0,i_1}^N\\alpha_iy_i\n",
    "$$\n",
    "To test for convergence of this algorithm, we can check whether the KKT conditions are satisfied to within some tolerance. \n",
    "There are more details that you can find in Platt's paper: \n",
    "\\begin{eqnarray}\n",
    "(1)&&\\mbox{Heuristics to choose $i_0$ and $i_1$}\\nonumber\\\\\n",
    "(2)&&\\mbox{How to update $b$}\\nonumber\n",
    "\\end{eqnarray}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
