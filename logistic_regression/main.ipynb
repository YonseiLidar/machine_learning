{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.06.24 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.06.15 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.09.24 PM.png\" width=\"100%\"></div>\n",
    "\n",
    "mathematicalmonk \n",
    "[ML 15.3](https://www.youtube.com/watch?v=-Z2a_mzl9LM&list=PLD0F06AA0D2E8FFBA&index=109) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.11.09 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.11.34 PM.png\" width=\"100%\"></div>\n",
    "\n",
    "mathematicalmonk \n",
    "[ML 15.4](https://www.youtube.com/watch?v=_7b352zYicY&list=PLD0F06AA0D2E8FFBA&index=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.13.08 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.14.28 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.16.41 PM.png\" width=\"100%\"></div>\n",
    "\n",
    "mathematicalmonk \n",
    "[ML 15.5](https://www.youtube.com/watch?v=hWLdFMccpTY&list=PLD0F06AA0D2E8FFBA&index=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.20.57 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.20.50 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.22.43 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.24.38 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.25.24 PM.png\" width=\"100%\"></div>\n",
    "\n",
    "mathematicalmonk \n",
    "[ML 15.6](https://www.youtube.com/watch?v=jUwjbiBUR-k&list=PLD0F06AA0D2E8FFBA&index=112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.34.17 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.39.41 PM.png\" width=\"100%\"></div>\n",
    "<div align=\"center\"><img src=\"img/Screen Shot 2019-09-15 at 3.42.49 PM.png\" width=\"100%\"></div>\n",
    "\n",
    "mathematicalmonk \n",
    "[ML 15.7](https://www.youtube.com/watch?v=X-7sA83PjPM&list=PLD0F06AA0D2E8FFBA&index=113)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l=-\\sum_{i=1}^n y^{(i)}\\log \\sigma^{(i)} + (1-y^{(i)})\\log (1-\\sigma^{(i)})\n",
    "$$\n",
    "where\n",
    "$$\\begin{array}{lll}\n",
    "\\sigma^{(i)}&=&\\sigma(z^{(i)})\\\\\n",
    "\\sigma(z)&=&\\frac{1}{1-e^{-z}}\\\\\n",
    "z^{(i)}&=&A[i,:]\\theta\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma'(z)=\\frac{e^{-z}}{(1-e^{-z})^2}=\\sigma(z)(1-\\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{lll}\n",
    "\\nabla l&=&-\\sum_{i=1}^n \\frac{y^{(i)}}{\\sigma^{(i)}}\\sigma^{(i)}(1-\\sigma^{(i)})A[i,:]^T - \\frac{(1-y^{(i)})}{(1-\\sigma^{(i)})}\\sigma^{(i)}(1-\\sigma^{(i)})A[i,:]^T\\\\\n",
    "&=&\\sum_{i=1}^n (\\sigma^{(i)}-y^{(i)})A[i,:]^T\\\\\n",
    "&=&(\\sigma^{(1)}-y^{(1)})\\left[\\begin{array}{c}1\\\\ x_1^{(1)}\\\\ x_2^{(1)}\\\\ x_3^{(1)}\\end{array}\\right]+\\cdots\n",
    "+(\\sigma^{(n)}-y^{(n)})\\left[\\begin{array}{c}1\\\\ x_1^{(n)}\\\\ x_2^{(n)}\\\\ x_3^{(n)}\\end{array}\\right]\\\\\n",
    "&=&\\left[\\begin{array}{ccc}1&\\cdots&1\\\\ x_1^{(1)}&\\cdots&x_1^{(n)}\\\\ x_2^{(1)}&\\cdots&x_2^{(n)}\\\\ x_3^{(1)}&\\cdots&x_3^{(n)}\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\\sigma^{(1)}-y^{(1)}\\\\ \\vdots\\\\ \\sigma^{(n)}-y^{(n)}\\end{array}\\right]\\\\\n",
    "&=&A^T(\\sigma-y)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{lll}\n",
    "\\nabla^2 l\n",
    "&=&\\sum_{i=1}^n \\sigma^{(i)}(1-\\sigma^{(i)})A[i,:]^TA[i,:]\\\\\n",
    "&=&\\sigma^{(1)}(1-\\sigma^{(1)})\\left[\\begin{array}{c}1\\\\ x_1^{(1)}\\\\ x_2^{(1)}\\\\ x_3^{(1)}\\end{array}\\right]\\left[\\begin{array}{cccc}1& x_1^{(1)}& x_2^{(1)}& x_3^{(1)}\\end{array}\\right]+\\cdots\n",
    "+\\sigma^{(n)}(1-\\sigma^{(n)})\\left[\\begin{array}{c}1\\\\ x_1^{(n)}\\\\ x_2^{(n)}\\\\ x_3^{(n)}\\end{array}\\right]\\left[\\begin{array}{cccc}1& x_1^{(n)}& x_2^{(n)}& x_3^{(n)}\\end{array}\\right]\\\\\n",
    "&=&\\left[\\begin{array}{ccc}1&\\cdots&1\\\\ x_1^{(1)}&\\cdots&x_1^{(n)}\\\\ x_2^{(1)}&\\cdots&x_2^{(n)}\\\\ x_3^{(1)}&\\cdots&x_3^{(n)}\\end{array}\\right]\n",
    "\\left[\\begin{array}{cccc}\\sigma^{(1)}(1-\\sigma^{(1)})&0&0&0\\\\\n",
    "0&\\sigma^{(2)}(1-\\sigma^{(2)})&0&0\\\\ \n",
    "0&0&\\ddots&0\\\\ \n",
    "0&0&0&\\sigma^{(n)}(1-\\sigma^{(n)})\\end{array}\\right]\n",
    "\\left[\\begin{array}{cccc}1&x_1^{(1)}&x_2^{(1)}&x_3^{(1)}\\\\\\vdots&\\vdots&\\vdots&\\vdots\\\\1&x_1^{(n)}&x_2^{(n)}&x_3^{(n)}\\end{array}\\right]\\\\\n",
    "&:=&A^TBA\\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{lll}\n",
    "\\theta&=&\\theta_0-H^{-1}g\\\\\n",
    "&=&\\theta_0-(A^TBA)^{-1}A^T(\\sigma-y)\\\\\n",
    "&=&(A^TBA)^{-1}A^TB(A\\theta_0-B^{-1}(\\sigma-y))\\\\\n",
    "&=&(A^TBA)^{-1}A^TBz\n",
    "\\end{array}$$\n",
    "where\n",
    "$$\n",
    "z=A\\theta_0-B^{-1}(\\sigma-y)\n",
    "$$\n",
    "\n",
    "We see that the update formula takes the form of a set of normal equations for a weighted least-squares problem. Because the weight matrix $B$ is not constant but depends on the parameter vector $\\theta$, we must apply the normal equations iteratively, each time using the new weight vector $\\theta$ to compute a revised weighting matrix $B$. For this reason, the argorithm is known as iterative reweighted least squares, or IRLS (Rubin, 1983). - Bishop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
